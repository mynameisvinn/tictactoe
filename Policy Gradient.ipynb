{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy gradient\n",
    "a policy gradient is a gradient-based approach to searching $\\theta$ for $\\pi$, which is $f: s_i \\mapsto a_i$. it is guided by a loss function of $\\mathbb{E} log(p(a_i) \\times (a_{chosen}) \\times reward$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Tictactoe import Tictactoe\n",
    "%matplotlib inline\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    return np.array([sum([gamma**t*r for t, r in enumerate(rewards[i:])]) for i in range(len(rewards))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 9\n",
    "n_actions = 9\n",
    "n_hidden = 256\n",
    "discount_factor = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "obs_ = tf.placeholder(name=\"observations\", shape=[None, n_features], dtype=tf.float32)\n",
    "a_ = tf.placeholder(name=\"actions\", shape=[None, n_actions], dtype=tf.float32)\n",
    "r_ = tf.placeholder(name=\"rewards\", shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_uniform([n_features, n_hidden]))\n",
    "w2 = tf.Variable(tf.random_uniform([n_hidden, n_actions]))\n",
    "\n",
    "w1 = tf.get_variable(\"w1\",[n_features, n_hidden])\n",
    "b1 = tf.get_variable(\"b1\",[n_hidden])\n",
    "w2 = tf.get_variable(\"w2\",[n_hidden, n_hidden])\n",
    "b2 = tf.get_variable(\"b2\",[n_hidden])\n",
    "w3 = tf.get_variable(\"w3\",[n_hidden, n_actions])\n",
    "b3 = tf.get_variable(\"b3\",[n_actions])\n",
    "\n",
    "z1 = tf.matmul(obs_, w1) + b1\n",
    "fc1 = tf.nn.relu(z1)\n",
    "z2 = tf.matmul(fc1, w2) + b2\n",
    "fc2 = tf.nn.relu(z2)\n",
    "z3 = tf.matmul(fc2, w3) + b3\n",
    "probs = tf.nn.softmax(z3)  # apply softmax on logits\n",
    "\n",
    "loss = tf.losses.log_loss(labels=a_, predictions=probs, weights=r_)\n",
    "op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_episodes = 50000\n",
    "    \n",
    "for episode in range(n_episodes):\n",
    "    env = Tictactoe(move_penalty=-0.2)\n",
    "    s1 = env.board.copy()\n",
    "    d = False\n",
    "    actions, rewards, states = [], [], []\n",
    "\n",
    "    while not d:\n",
    "        action_probs = sess.run(probs, feed_dict={obs_: s1.reshape(1, -1)})[0]\n",
    "        available = env.available_actions()\n",
    "        a = np.random.choice(available, p=action_probs[available]/np.sum(action_probs[available]))\n",
    "        # a = env.sample()\n",
    "        s2, r, d = env.step(a)\n",
    "\n",
    "        actions.append(np.eye(n_actions)[a])\n",
    "        rewards.append(r)\n",
    "        states.append(s1)\n",
    "        \n",
    "        s1 = s2.copy()\n",
    "\n",
    "    epr = np.vstack(discount_rewards(rewards, discount_factor))\n",
    "    eps = np.vstack(states)\n",
    "    epl = np.vstack(actions)\n",
    "    epr -= np.mean(epr)\n",
    "    epr /= np.std(epr)\n",
    "\n",
    "    sess.run(op, {obs_: eps, a_: epl, r_: epr})\n",
    "\n",
    "    if episode % (n_episodes / 10) == 0:\n",
    "        print (episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wins', 0.94)\n",
      "('losses', 0.06)\n",
      "('draws', 0.0)\n"
     ]
    }
   ],
   "source": [
    "wins = 0.0\n",
    "losses = 0.0\n",
    "draws = 0.0\n",
    "rollouts = 100\n",
    "\n",
    "for _ in range(rollouts):\n",
    "    \n",
    "    env = Tictactoe()\n",
    "    s = env.board\n",
    "    d = False\n",
    "\n",
    "    while not d:\n",
    "        action_probs = sess.run(probs, feed_dict={obs_: s.reshape(1, -1)})[0]\n",
    "        available = env.available_actions()\n",
    "        a = np.random.choice(available, p=action_probs[available]/np.sum(action_probs[available]))\n",
    "        s, r, d = env.step(a)\n",
    "\n",
    "    if r == 1:\n",
    "        wins += 1\n",
    "    elif r == -1:\n",
    "        losses += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "        \n",
    "print (\"wins\", wins/rollouts)\n",
    "print (\"losses\", losses/rollouts)\n",
    "print (\"draws\", draws/rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
